<!DOCTYPE HTML>
<html>
<section class="thirteen columns">
    <h1>Publications</h1>

<head>
<title>JabRef references</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<script type="text/javascript">
<!--
// QuickSearch script for JabRef HTML export
// Version: 3.0
//
// Copyright (c) 2006-2011, Mark Schenk
//
// This software is distributed under a Creative Commons Attribution 3.0 License
// http://creativecommons.org/licenses/by/3.0/
//
// Features:
// - intuitive find-as-you-type searching
//    ~ case insensitive
//    ~ ignore diacritics (optional)
//
// - search with/without Regular Expressions
// - match BibTeX key
//

// Search settings
var searchAbstract = true;	// search in abstract
var searchReview = true;	// search in review

var noSquiggles = true; 	// ignore diacritics when searching
var searchRegExp = false; 	// enable RegExp searches


if (window.addEventListener) {
	window.addEventListener("load",initSearch,false); }
else if (window.attachEvent) {
	window.attachEvent("onload", initSearch); }

function initSearch() {
	// check for quick search table and searchfield
	if (!document.getElementById('qs_table')||!document.getElementById('quicksearch')) { return; }

	// load all the rows and sort into arrays
	loadTableData();

	//find the query field
	qsfield = document.getElementById('qs_field');

	// previous search term; used for speed optimisation
	prevSearch = '';

	//find statistics location
	stats = document.getElementById('stat');
	setStatistics(-1);

	// set up preferences
	initPreferences();

	// shows the searchfield
	document.getElementById('quicksearch').style.display = 'block';
	document.getElementById('qs_field').onkeyup = quickSearch;
}

function loadTableData() {
	// find table and appropriate rows
	searchTable = document.getElementById('qs_table');
	var allRows = searchTable.getElementsByTagName('tbody')[0].getElementsByTagName('tr');

	// split all rows into entryRows and infoRows (e.g. abstract, review, bibtex)
	entryRows = new Array(); infoRows = new Array(); absRows = new Array(); revRows = new Array();

	// get data from each row
	entryRowsData = new Array(); absRowsData = new Array(); revRowsData = new Array();

	BibTeXKeys = new Array();

	for (var i=0, k=0, j=0; i<allRows.length;i++) {
		if (allRows[i].className.match(/entry/)) {
			entryRows[j] = allRows[i];
			entryRowsData[j] = stripDiacritics(getTextContent(allRows[i]));
			allRows[i].id ? BibTeXKeys[j] = allRows[i].id : allRows[i].id = 'autokey_'+j;
			j ++;
		} else {
			infoRows[k++] = allRows[i];
			// check for abstract/review
			if (allRows[i].className.match(/abstract/)) {
				absRows.push(allRows[i]);
				absRowsData[j-1] = stripDiacritics(getTextContent(allRows[i]));
			} else if (allRows[i].className.match(/review/)) {
				revRows.push(allRows[i]);
				revRowsData[j-1] = stripDiacritics(getTextContent(allRows[i]));
			}
		}
	}
	//number of entries and rows
	numEntries = entryRows.length;
	numInfo = infoRows.length;
	numAbs = absRows.length;
	numRev = revRows.length;
}

function quickSearch(){

	tInput = qsfield;

	if (tInput.value.length == 0) {
		showAll();
		setStatistics(-1);
		qsfield.className = '';
		return;
	} else {
		t = stripDiacritics(tInput.value);

		if(!searchRegExp) { t = escapeRegExp(t); }

		// only search for valid RegExp
		try {
			textRegExp = new RegExp(t,"i");
			closeAllInfo();
			qsfield.className = '';
		}
			catch(err) {
			prevSearch = tInput.value;
			qsfield.className = 'invalidsearch';
			return;
		}
	}

	// count number of hits
	var hits = 0;

	// start looping through all entry rows
	for (var i = 0; cRow = entryRows[i]; i++){

		// only show search the cells if it isn't already hidden OR if the search term is getting shorter, then search all
		if(cRow.className.indexOf('noshow')==-1 || tInput.value.length <= prevSearch.length){
			var found = false;

			if (entryRowsData[i].search(textRegExp) != -1 || BibTeXKeys[i].search(textRegExp) != -1){
				found = true;
			} else {
				if(searchAbstract && absRowsData[i]!=undefined) {
					if (absRowsData[i].search(textRegExp) != -1){ found=true; }
				}
				if(searchReview && revRowsData[i]!=undefined) {
					if (revRowsData[i].search(textRegExp) != -1){ found=true; }
				}
			}

			if (found){
				cRow.className = 'entry show';
				hits++;
			} else {
				cRow.className = 'entry noshow';
			}
		}
	}

	// update statistics
	setStatistics(hits)

	// set previous search value
	prevSearch = tInput.value;
}


// Strip Diacritics from text
// http://stackoverflow.com/questions/990904/javascript-remove-accents-in-strings

// String containing replacement characters for stripping accents
var stripstring =
    'AAAAAAACEEEEIIII'+
    'DNOOOOO.OUUUUY..'+
    'aaaaaaaceeeeiiii'+
    'dnooooo.ouuuuy.y'+
    'AaAaAaCcCcCcCcDd'+
    'DdEeEeEeEeEeGgGg'+
    'GgGgHhHhIiIiIiIi'+
    'IiIiJjKkkLlLlLlL'+
    'lJlNnNnNnnNnOoOo'+
    'OoOoRrRrRrSsSsSs'+
    'SsTtTtTtUuUuUuUu'+
    'UuUuWwYyYZzZzZz.';

function stripDiacritics(str){

    if(noSquiggles==false){
        return str;
    }

    var answer='';
    for(var i=0;i<str.length;i++){
        var ch=str[i];
        var chindex=ch.charCodeAt(0)-192;   // Index of character code in the strip string
        if(chindex>=0 && chindex<stripstring.length){
            // Character is within our table, so we can strip the accent...
            var outch=stripstring.charAt(chindex);
            // ...unless it was shown as a '.'
            if(outch!='.')ch=outch;
        }
        answer+=ch;
    }
    return answer;
}

// http://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex
// NOTE: must escape every \ in the export code because of the JabRef Export...
function escapeRegExp(str) {
  return str.replace(/[-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, "\\$&");
}

function toggleInfo(articleid,info) {

	var entry = document.getElementById(articleid);
	var abs = document.getElementById('abs_'+articleid);
	var rev = document.getElementById('rev_'+articleid);
	var bib = document.getElementById('bib_'+articleid);

	if (abs && info == 'abstract') {
		abs.className.indexOf('noshow') == -1?abs.className = 'abstract noshow':abs.className = 'abstract show';
	} else if (rev && info == 'review') {
		rev.className.indexOf('noshow') == -1?rev.className = 'review noshow':rev.className = 'review show';
	} else if (bib && info == 'bibtex') {
		bib.className.indexOf('noshow') == -1?bib.className = 'bibtex noshow':bib.className = 'bibtex show';
	} else {
		return;
	}

	// check if one or the other is available
	var revshow; var absshow; var bibshow;
	(abs && abs.className.indexOf('noshow') == -1)? absshow = true: absshow = false;
	(rev && rev.className.indexOf('noshow') == -1)? revshow = true: revshow = false;
	(bib && bib.className.indexOf('noshow') == -1)? bibshow = true: bibshow = false;

	// highlight original entry
	if(entry) {
		if (revshow || absshow || bibshow) {
		entry.className = 'entry highlight show';
		} else {
		entry.className = 'entry show';
		}
	}

	// When there's a combination of abstract/review/bibtex showing, need to add class for correct styling
	if(absshow) {
		(revshow||bibshow)?abs.className = 'abstract nextshow':abs.className = 'abstract';
	}
	if (revshow) {
		bibshow?rev.className = 'review nextshow': rev.className = 'review';
	}

}

function setStatistics (hits) {
	if(hits < 0) { hits=numEntries; }
	if(stats) { stats.firstChild.data = hits + '/' + numEntries}
}

function getTextContent(node) {
	// Function written by Arve Bersvendsen
	// http://www.virtuelvis.com

	if (node.nodeType == 3) {
	return node.nodeValue;
	} // text node
	if (node.nodeType == 1 && node.className != "infolinks") { // element node
	var text = [];
	for (var chld = node.firstChild;chld;chld=chld.nextSibling) {
		text.push(getTextContent(chld));
	}
	return text.join("");
	} return ""; // some other node, won't contain text nodes.
}

function showAll(){
	closeAllInfo();
	for (var i = 0; i < numEntries; i++){ entryRows[i].className = 'entry show'; }
}

function closeAllInfo(){
	for (var i=0; i < numInfo; i++){
		if (infoRows[i].className.indexOf('noshow') ==-1) {
			infoRows[i].className = infoRows[i].className + ' noshow';
		}
	}
}

function clearQS() {
	qsfield.value = '';
	showAll();
}

function redoQS(){
	showAll();
	quickSearch(qsfield);
}

function updateSetting(obj){
	var option = obj.id;
	var checked = obj.value;

	switch(option)
	 {
	 case "opt_searchAbs":
	   searchAbstract=!searchAbstract;
	   redoQS();
	   break;
	 case "opt_searchRev":
	   searchReview=!searchReview;
	   redoQS();
	   break;
	 case "opt_useRegExp":
	   searchRegExp=!searchRegExp;
	   redoQS();
	   break;
	 case "opt_noAccents":
	   noSquiggles=!noSquiggles;
	   loadTableData();
	   redoQS();
	   break;
	 }
}

function initPreferences(){
	if(searchAbstract){document.getElementById("opt_searchAbs").checked = true;}
	if(searchReview){document.getElementById("opt_searchRev").checked = true;}
	if(noSquiggles){document.getElementById("opt_noAccents").checked = true;}
	if(searchRegExp){document.getElementById("opt_useRegExp").checked = true;}

	if(numAbs==0) {document.getElementById("opt_searchAbs").parentNode.style.display = 'none';}
	if(numRev==0) {document.getElementById("opt_searchRev").parentNode.style.display = 'none';}
}

function toggleSettings(){
	var togglebutton = document.getElementById('showsettings');
	var settings = document.getElementById('settings');

	if(settings.className == "hidden"){
		settings.className = "show";
		togglebutton.innerText = "close settings";
		togglebutton.textContent = "close settings";
	}else{
		settings.className = "hidden";
		togglebutton.innerText = "settings...";
		togglebutton.textContent = "settings...";
	}
}

-->
</script>
<style type="text/css">
body { background-color: white; font-family: Arial, sans-serif; font-size: 13px; line-height: 1.2; padding: 1em; color: #2E2E2E; margin: auto auto; }

form#quicksearch { width: auto; border-style: solid; border-color: gray; border-width: 1px 0px; padding: 0.7em 0.5em; display:none; position:relative; }
span#searchstat {padding-left: 1em;}

div#settings { margin-top:0.7em; /* border-bottom: 1px transparent solid; background-color: #efefef; border: 1px grey solid; */ }
div#settings ul {margin: 0; padding: 0; }
div#settings li {margin: 0; padding: 0 1em 0 0; display: inline; list-style: none; }
div#settings li + li { border-left: 2px #efefef solid; padding-left: 0.5em;}
div#settings input { margin-bottom: 0px;}

div#settings.hidden {display:none;}

#showsettings { border: 1px grey solid; padding: 0 0.5em; float:right; line-height: 1.6em; text-align: right; }
#showsettings:hover { cursor: pointer; }

.invalidsearch { background-color: red; }
input[type="button"] { background-color: #efefef; border: 1px #2E2E2E solid;}

table { border: 1px gray none; width: 100%; empty-cells: show; border-spacing: 0em 0.1em; margin: 1em 0em; }
th, td { border: none; padding: 0.5em; vertical-align: top; text-align: justify; }

td a { color: navy; text-decoration: none; }
td a:hover  { text-decoration: underline; }

tr.noshow { display: none;}
tr.highlight td { background-color: #EFEFEF; border-top: 2px #2E2E2E solid; font-weight: bold; }
tr.abstract td, tr.review td, tr.bibtex td { background-color: #EFEFEF; text-align: justify; border-bottom: 2px #2E2E2E solid; }
tr.nextshow td { border-bottom-style: none; }

tr.bibtex pre { width: 100%; overflow: auto; white-space: pre-wrap;}
p.infolinks { margin: 0.3em 0em 0em 0em; padding: 0px; }

@media print {
	p.infolinks, #qs_settings, #quicksearch, t.bibtex { display: none !important; }
	tr { page-break-inside: avoid; }
}
</style>
</head>
<body>

<table id="qs_table" border="1">
<tbody>
</tr>
<tr id="Heiden2020" class="entry">
	<td>Heiden Tvd, <strong>Mirus F</strong>, and Hoof Hv (2020), <i>"Social navigation with human empowerment driven reinforcement learning"</i>. International Conference on Artifical Neural Networks (ICANN) 2020, accepted for publication.
	<p class="infolinks">[<a href="javascript:toggleInfo('Heiden2020','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Heiden2020','bibtex')">BibTeX</a>] [<a href="https://arxiv.org/pdf/2003.08158.pdf" target="_blank">pdf</a>]</p>
	</td>
</tr>
<tr id="abs_Heiden2020" class="abstract noshow">
	<td>
        Mobile robot navigation has seen extensive research in the last decades. The aspect of collaboration with robots and humans sharing workspaces will become increasingly important in the future. Therefore, the next generation of mobile robots needs to be socially-compliant to be accepted by their human collaborators. 
        However, a formal definition of compliance is not straightforward. On the other hand, empowerment has been used by artificial agents to learn complicated and generalized actions and also has been shown to be a good model for biological behaviors. In this paper, we go beyond the approach of classical \acf{RL} and provide our agent with intrinsic motivation using empowerment. In contrast to self-empowerment, a robot employing our approach strives for the empowerment of people in its environment, so they are not disturbed by the robot's presence and motion. In our experiments, we show that our approach has a positive influence on humans, as it minimizes its distance to humans and thus decreases human travel time while moving efficiently towards its own goal. An interactive user-study shows that our method is considered more social than other state-of-the-art approaches by the participants.

    </td>
</tr>
<tr id="bib_Heiden2020" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@InProceedings{Heiden2020,
  author    = {Tessa van der Heiden and Florian Mirus and Herke van Hoof},
  booktitle = {International Conference on Artificial Neural Networks (ICANN) 2020},
  title     = {Social navigation with human empowerment driven reinforcement learning},
  publisher = {Springer International Publishing},
  abstract  = {Mobile robot navigation has seen extensive research in the last decades. The aspect of collaboration with robots and humans sharing workspaces will become increasingly important in the future. Therefore, the next generation of mobile robots needs to be socially-compliant to be accepted by their human collaborators. However, a formal definition of compliance is not straightforward. On the other hand, empowerment has been used by artificial agents to learn complicated and generalized actions and also has been shown to be a good model for biological behaviors. In this paper, we go beyond the approach of classical \acf{RL} and provide our agent with intrinsic motivation using empowerment. In contrast to self-empowerment, a robot employing our approach strives for the empowerment of people in its environment, so they are not disturbed by the robot's presence and motion. In our experiments, we show that our approach has a positive influence on humans, as it minimizes its distance to humans and thus decreases human travel time while moving efficiently towards its own goal. An interactive user-study shows that our method is considered more social than other state-of-the-art approaches by the participants.
  },
  date      = {2020},
  note      = {accepted for publication},
}
</pre></td>
</tr>

<tr id="Mirus2020a" class="entry">
	<td><strong>Mirus F</strong>, Stewart TC and Conradt J (2020), <i>"The Importance of Balanced Data Sets: Analyzing a Vehicle Trajectory Prediction Model based on Neural Networks and Distributed Representations"</i>, In IEEE International Joint Conference on Neural Networks (IJCNN) 2020, Glasgow, Scotland, doi: 10.1109/IJCNN48605.2020.9206627 
	<p class="infolinks">[<a href="javascript:toggleInfo('Mirus2020a','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Mirus2020a','bibtex')">BibTeX</a>] [<a href="https://doi.org/10.1109/IJCNN48605.2020.9206627" target="_blank">DOI</a>] [<a href="https://ieeexplore.ieee.org/document/9206627" target="_blank">URL</a>] [<a href="https://arxiv.org/pdf/2010.00084.pdf" target="_blank">pdf</a>]</p>
	</td>
</tr>
<tr id="abs_Mirus2020a" class="abstract noshow">
	<td><b>Abstract</b>: Predicting future behavior of other traffic participants is an essential task that needs to be solved by automated vehicles and human drivers alike to achieve safe and situationaware driving. Modern approaches to vehicles trajectory prediction typically rely on data-driven models like neural networks, in particular LSTMs (Long Short-Term Memorys), achieving promising results. However, the question of optimal composition of the underlying training data has received less attention. In this paper, we expand on previous work on vehicle trajectory prediction based on neural network models employing distributed representations to encode automotive scenes in a semantic vector substrate. We analyze the influence of variations in the training data on the performance of our prediction models. Thereby, we show that the models employing our semantic vector representation outperform the numerical model when trained on an adequate data set and thereby, that the composition of training data in vehicle trajectory prediction is crucial for successful training. We conduct our analysis on challenging real-world driving data.
</tr>
<tr id="bib_Mirus2020a" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@InProceedings{Mirus2020a,
  author    = {Florian Mirus and Terrence C. Stewart and J\"org Conradt},
  booktitle = {2020 International Joint Conference on Neural Networks (IJCNN)},
  title     = {The Importance of Balanced Data Sets: Analyzing a Vehicle Trajectory Prediction Model based on Neural Networks and Distributed Representations},
  month     = {jul},
  year      = {2020},
  publisher = {{IEEE}},
  doi       = {10.1109/IJCNN48605.2020.9206627},
  date      = {2020-07-19},
  pages     = {1--8},
}
</pre></td>
</tr>
<tr id="Mirus2020b" class="entry">
	<td><strong>Mirus F</strong>, Stewart TC and Conradt J (2020), <i>"Analyzing the Capacity of Distributed Vector Representations to Encode Spatial Information"</i>, In IEEE International Joint Conference on Neural Networks (IJCNN) 2020, Glasgow, Scotland, doi: 10.1109/IJCNN48605.2020.9207137 
	<p class="infolinks">[<a href="javascript:toggleInfo('Mirus2020b','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Mirus2020b','bibtex')">BibTeX</a>] [<a href="https://doi.org/10.1109/IJCNN48605.2020.9207137" target="_blank">DOI</a>] [<a href="https://ieeexplore.ieee.org/document/9207137" target="_blank">URL</a>] [<a href="https://arxiv.org/pdf/2010.00055.pdf" target="_blank">pdf</a>] [<a href="https://github.com/fmirus/spa_capacity_analysis" target="_blank">Code</a>]</p>
	</td>
</tr>
<tr id="abs_Mirus2020b" class="abstract noshow">
	<td><b>Abstract</b>: Vector Symbolic Architectures belong to a family of related cognitive modeling approaches that encode symbols and structures in high-dimensional vectors. Similar to human subjects, whose capacity to process and store information or concepts in short-term memory is subject to numerical restrictions, the capacity of information that can be encoded in such vector representations is limited and one way of modeling the numerical restrictions to cognition. In this paper, we analyze these limits regarding information capacity of distributed representations. We focus our analysis on simple superposition and more complex, structured representations involving convolutive powers to encode spatial information. In two experiments, we find upper bounds for the number of concepts that can effectively be stored in a single vector.
</tr>
<tr id="bib_Mirus2020b" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@InProceedings{Mirus2020b,
  author    = {Florian Mirus and Terrence C. Stewart and J\"org Conradt},
  booktitle = {2020 International Joint Conference on Neural Networks (IJCNN)},
  title     = {Analyzing the Capacity of Distributed Vector Representations to Encode Spatial Information},
  month     = {jul},
  year      = {2020},
  publisher = {{IEEE}},
  doi       = {10.1109/IJCNN48605.2020.9207137},
  date      = {2020-07-19},
  pages     = {1--7},
}
</pre></td>
</tr>
<tr id="Mirus2020" class="entry">
	<td><strong>Mirus F</strong>, Stewart TC and Conradt J (2020), <i>"Detection of abnormal driving situations using distributed representations and unsupervised learning"</i>, In 28th European Symposium on Artificial Neural Networks, ESANN 2020, Bruges, Belgium
	<p class="infolinks">[<a href="javascript:toggleInfo('Mirus2020','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Mirus2020','bibtex')">BibTeX</a>] [<a href="https://www.esann.org/sites/default/files/proceedings/2020/ES2020-81.pdf" target="_blank">pdf</a>]</p>
	</td>
</tr>
<tr id="abs_Mirus2020" class="abstract noshow">
	<td><b>Abstract</b>: In this paper, we present an anomaly detection system employing an unsupervised learning model trained on the information encapsulated within distributed vector representations of automotive scenes.  Our representations allows us to encode automotive scenes with a varying number of traffic participants in a vector of fixed length. We train a neural network autoencoder in unsupervised fashion to detect anomalies based on this representation. We demonstrate the usefulness of our approach through a quantitative analysis on two real-world data-sets.
</tr>
<tr id="bib_Mirus2020" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@InProceedings{Mirus2020,
  author    = {Florian Mirus and Terrence C. Stewart and J\"org Conradt},
  booktitle = {28th European Symposium on Artificial Neural Networks, {ESANN} 2020, Bruges, Belgium},
  title     = {Detection of abnormal driving situations using distributed representations and unsupervised learning},
  year      = {2020},
  date      = {2020-04-23},
}
</pre></td>
</tr>
<tr id="Mirus2019b" class="entry">
	<td><strong>Mirus F</strong>, Blouw P, Stewart TC, and Conradt J (2019), <i>"An Investigation of Vehicle Behavior Prediction Using a Vector Power Representation to Encode Spatial Positions of Multiple Objects and Neural Networks"</i>. Front. Neurorobot. 13:84. doi: 10.3389/fnbot.2019.00084
	<p class="infolinks">[<a href="javascript:toggleInfo('Mirus2019b','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Mirus2019b','bibtex')">BibTeX</a>][<a href="https://doi.org/10.3389/fnbot.2019.00084" target="_blank">DOI</a>] [<a href="https://www.frontiersin.org/articles/10.3389/fnbot.2019.00084/full" target="_blank">URL</a>]</p>
	</td>
</tr>
<tr id="abs_Mirus2019b" class="abstract noshow">
	<td>
        Predicting future behavior and positions of other traffic participants from observations is a key problem that needs to be solved by human drivers and automated vehicles alike to safely navigate their environment and to reach their desired goal.
        In this paper, we expand on previous work on an automotive environment model based on vector symbolic architectures (VSAs).
        We investigate a vector-representation to encapsulate spatial information of multiple objects based on a convolutive power encoding.
        Assuming that future positions of vehicles are influenced not only by their own past positions and dynamics (e.g., velocity and acceleration) but also by the behavior of the other traffic participants in the vehicle's surroundings, our motivation is 3-fold: we hypothesize that our structured vector-representation will be able to capture these relations and mutual influence between multiple traffic participants.
        Furthermore, the dimension of the encoding vectors remains fixed while being independent of the number of other vehicles encoded in addition to the target vehicle.
        Finally, a VSA-based encoding allows us to combine symbol-like processing with the advantages of neural network learning.
        In this work, we use our vector representation as input for a long short-term memory (LSTM) network for sequence to sequence prediction of vehicle positions.
        In an extensive evaluation, we compare this approach to other LSTM-based benchmark systems using alternative data encoding schemes, simple feed-forward neural networks as well as a simple linear prediction model for reference.
        We analyze advantages and drawbacks of the presented methods and identify specific driving situations where our approach performs best.
        We use characteristics specifying such situations as a foundation for an online-learning mixture-of-experts prototype, which chooses at run time between several available predictors depending on the current driving situation to achieve the best possible forecast.
    </td>
</tr>
<tr id="bib_Mirus2019b" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@Article{Mirus2019b,
  author       = {Mirus, Florian and Blouw, Peter and Stewart, Terrence C. and Conradt, Jörg},
  title        = {An Investigation of Vehicle Behavior Prediction Using a Vector Power Representation to Encode Spatial Positions of Multiple Objects and Neural Networks},
  issn         = {1662-5218},
  month        = {oct},
  pages        = {84},
  volume       = {13},
  abstract     = {Predicting future behavior and positions of other traffic participants from observations is a key problem that needs to be solved by human drivers and automated vehicles alike to safely navigate their environment and to reach their desired goal. In this paper, we expand on previous work on an automotive environment model based on vector symbolic architectures (VSAs). We investigate a vector-representation to encapsulate spatial information of multiple objects based on a convolutive power encoding. Assuming that future positions of vehicles are influenced not only by their own past positions and dynamics (e.g., velocity and acceleration) but also by the behavior of the other traffic participants in the vehicle's surroundings, our motivation is 3-fold: we hypothesize that our structured vector-representation will be able to capture these relations and mutual influence between multiple traffic participants. Furthermore, the dimension of the encoding vectors remains fixed while being independent of the number of other vehicles encoded in addition to the target vehicle. Finally, a VSA-based encoding allows us to combine symbol-like processing with the advantages of neural network learning. In this work, we use our vector representation as input for a long short-term memory (LSTM) network for sequence to sequence prediction of vehicle positions. In an extensive evaluation, we compare this approach to other LSTM-based benchmark systems using alternative data encoding schemes, simple feed-forward neural networks as well as a simple linear prediction model for reference. We analyze advantages and drawbacks of the presented methods and identify specific driving situations where our approach performs best. We use characteristics specifying such situations as a foundation for an online-learning mixture-of-experts prototype, which chooses at run time between several available predictors depending on the current driving situation to achieve the best possible forecast.},
  date         = {2019},
  journaltitle = {Frontiers in Neurorobotics},
  doi          = {10.3389/fnbot.2019.00084},
}
</pre></td>
</tr>
<tr id="Mirus2019c" class="entry">
	<td><strong>Mirus F</strong>, Stewart TC, Eliasmith C, and Conradt J (2019), <i>"A Mixture-of-Experts Model for Vehicle Prediction Using an Online Learning Approach"</i>. In: Tetko I., Kůrková V., Karpov P., Theis F. (eds) Artificial Neural Networks and Machine Learning – ICANN 2019: Image Processing. ICANN 2019. Lecture Notes in Computer Science, vol 11729. Springer, Cham.
	<p class="infolinks">[<a href="javascript:toggleInfo('Mirus2019c','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Mirus2019c','bibtex')">BibTeX</a>][<a href="https://doi.org/10.1007/978-3-030-30508-6_37" target="_blank">DOI</a>] [<a href="https://link.springer.com/chapter/10.1007/978-3-030-30508-6_37" target="_blank">URL</a>]</p>
	</td>
</tr>
<tr id="abs_Mirus2019c" class="abstract noshow">
	<td>
       Predicting future motion of other vehicles or, more generally, the development of traffic situations is an essential step towards secure, context-aware automated driving. Human drivers are able to anticipate driving situations continuously based on the currently perceived behavior of other traffic participants while incorporating prior experience. The most successful data-driven prediction models are typically trained on large amounts of recorded data before deployment achieving remarkable results. In this paper, we present a mixture-of-experts online learning model encapsulating both ideas. Our system learns  at run time to choose between several models, which have been previously trained offline, based on the current situational context. We show that our model is able to improve over the offline models already after a short ramp-up phase. We evaluate our system on real world driving data. 
    </td>
</tr>
<tr id="bib_Mirus2019c" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@InProceedings{Mirus2019c,
  author    = {Mirus, Florian and Stewart, Terrence C. and Eliasmith, Chris and Conradt, J{\"o}rg},
  booktitle = {Artificial Neural Networks and Machine Learning -- ICANN 2019: Image Processing},
  title     = {A Mixture-of-Experts Model for Vehicle Prediction Using an Online Learning Approach},
  editor    = {Tetko, Igor V. and K{\r{u}}rkov{\'a}, V{\v{e}}ra and Karpov, Pavel and Theis, Fabian},
  pages     = {456--471},
  publisher = {Springer International Publishing},
  series    = {Lecture Notes in Computer Science},
  volume    = {11729},
  abstract  = {Predicting future motion of other vehicles or, more generally, the development of traffic situations, is an essential step towards secure, context-aware automated driving. On the one hand, human drivers are able to anticipate driving situations continuously based on the currently perceived behavior of other traffic participants while incorporating prior experience. On the other hand, the most successful data-driven prediction models are typically trained on large amounts of recorded data before deployment achieving remarkable results. In this paper, we present a mixture-of-experts online learning model encapsulating both ideas. Our system learns at run time to choose between several models, which have been previously trained offline, based on the current situational context. We show that our model is able to improve over the offline models already after a short ramp-up phase. We evaluate our system on real world driving data.},
  date      = {2019},
  doi       = {10.1007/978-3-030-30508-6_37},
  isbn      = {978-3-030-30508-6},
}
</pre></td>
</tr>
<tr id="Mirus2019" class="entry">
	<td><strong>Mirus F</strong>, Blouw P, Stewart TC and Conradt J (2019), <i>"Predicting vehicle behaviour using LSTMs and a vector power representation for spatial positions"</i>, In 27th European Symposium on Artificial Neural Networks, ESANN 2019, Bruges, Belgium, pp. 113-118.
	<p class="infolinks">[<a href="javascript:toggleInfo('Mirus2019','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Mirus2019','bibtex')">BibTeX</a>][<a href="{{ site.baseurl }}static/docs/2019_ESANN/2019_ESANN_spa_paper.pdf" target="_blank">Paper pdf</a>] [<a href="{{ site.baseurl }}static/docs/2019_ESANN/2019_ESANN_spa_poster.pdf" target="_blank">Poster pdf</a>]</p>
	</td>
</tr>
<tr id="abs_Mirus2019" class="abstract noshow">
	<td><b>Abstract</b>: Predicting future vehicle behaviour is an essential task to enable safe and situation-aware automated driving. In this paper, we propose to encapsulate spatial information of multiple objects in a semantic vector-representation. Assuming that future vehicle motion is influenced not only by past positions but also by the behaviour of other traffic participants, we use this representation as input for a Long Short-Term Memory (LSTM) network for sequence to sequence prediction of vehicle positions. We train and evaluate our system on real-world driving data collected mainly on highways in southern Germany and compare it to other models for reference.</td>
</tr>
<tr id="bib_Mirus2019" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{Mirus2019,
  author = {Florian Mirus and Peter Blouw and Terrence C. Stewart and J&ouml;rg Conradt},
  title = {Predicting vehicle behaviour using LSTMs and a vector power representation for spatial positions},
  booktitle = {27th European Symposium on Artificial Neural Networks, {ESANN} 2019, Bruges, Belgium},
  year = {2019},
  pages = {113--118}
}
</pre></td>
</tr>
<tr id="Mirus2019a" class="entry">
	<td><strong>Mirus F</strong>, Zorn B and Conradt B (2019), <i>"Short-term trajectory planning using reinforcement learning within a neuromorphic control architecture"</i>, In 27th European Symposium on Artificial Neural Networks, ESANN 2019, Bruges, Belgium, pp. 649-654.
	<p class="infolinks">[<a href="javascript:toggleInfo('Mirus2019a','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Mirus2019a','bibtex')">BibTeX</a>][<a href="{{ site.baseurl }}static/docs/2019_ESANN/2019_ESANN_torcs_paper.pdf" target="_blank">Paper pdf</a>] [<a href="{{ site.baseurl }}static/docs/2019_ESANN/2019_ESANN_torcs_poster.pdf" target="_blank">Poster pdf</a>]</p>
	</td>
</tr>
<tr id="abs_Mirus2019a" class="abstract noshow">
	<td><b>Abstract</b>: In this paper, we present a first step towards neuromorphic vehicle control. We propose a modular and hierarchical system architecture entirely implemented in a spiking neuron substrate, which allows for adjustment of individual components through either supervised or reinforcement learning as well as future deployment on dedicated neuromorphic hardware. In a sample instantiation, we investigate automated training of a neuromorphic trajectory selection module using reinforcement learning to demonstrate the general feasibility of our approach. We evaluate our system using the open-source race car simulator TORCS.</td>
</tr>
<tr id="bib_Mirus2019a" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{Mirus2018,
  author = {Florian Mirus and Benjamin Zorn and J&ouml;rg Conradt},
  title = {Short-term trajectory planning using reinforcement learning within a neuromorphic control architecture},
  booktitle = {27th European Symposium on Artificial Neural Networks, ESANN 2019, Bruges, Belgium},
  year = {2019},
  pages = {649--654}
}
</pre></td>
</tr>
<tr id="Mirus2018" class="entry">
	<td><strong>Mirus F</strong>, Stewart TC and Conradt J (2018), <i>"Towards cognitive automotive environment modelling: reasoning based on vector representations"</i>, In 26th European Symposium on Artificial Neural Networks, ESANN 2018, Bruges, Belgium, pp. 55-60.
	<p class="infolinks">[<a href="javascript:toggleInfo('Mirus2018','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Mirus2018','bibtex')">BibTeX</a>] [<a href="{{ site.baseurl }}static/docs/2018_ESANN/2018_ESANN_paper.pdf" target="_blank">Paper pdf</a>] [<a href="{{ site.baseurl }}static/docs/2018_ESANN/2018_ESANN_poster.pdf" target="_blank">Poster pdf</a>]</p>
	</td>
</tr>
<tr id="abs_Mirus2018" class="abstract noshow">
	<td><b>Abstract</b>: In this paper, we propose a novel approach to knowledge representation for automotive environment modelling based on Vector Symbolic Architectures (VSAs). We build a vector representation describing structured information and relations within the current scene based on high-level object-lists perceived by individual sensors. Such a representation can be applied to different tasks with little modifications. In a sample instantiation, we focus on two example tasks, namely driving context classification and simple behavior prediction, to demonstrate the general applicability of our approach. Allowing efficient implementation in Spiking Neural Networks (SNNs), we envision to improve task performance of our approach through online-learning.</td>
</tr>
<tr id="bib_Mirus2018" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{Mirus2018,
  author = {Florian Mirus and Terrence C. Stewart and J&ouml;rg Conradt},
  title = {Towards cognitive automotive environment modelling: reasoning based on vector representations},
  booktitle = {26th European Symposium on Artificial Neural Networks, ESANN 2018, Bruges, Belgium},
  year = {2018},
  Pages = {55--60}
}
</pre></td>
</tr>

<tr id="Mirus2018a" class="entry">
	<td><strong>Mirus F</strong>, Axenie C, Stewart TC and Conradt J (2018), <i>"Neuromorphic Sensorimotor Adaptation for Robotic Mobile Manipulation: From Sensing to Behaviour "</i>, Cognitive Systems Research, pp. 52-66.
	<p class="infolinks">[<a href="javascript:toggleInfo('Mirus2018a','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Mirus2018a','bibtex')">BibTeX</a>] [<a href="https://doi.org/10.1016/j.cogsys.2018.03.006" target="_blank">DOI</a>] [<a href="https://www.sciencedirect.com/science/article/pii/S1389041717300955" target="_blank">URL</a>] [<a href="https://github.com/fmirus/nst_omni_sort" target="_blank">Code</a>]</p>
	</td>
</tr>
<tr id="abs_Mirus2018a" class="abstract noshow">
	<td><b>Abstract</b>: We propose a neuromorphic approach to perception, reasoning and motor control using Spiking Neural Networks in mobile robotics. We demonstrate this by using a mobile robotic manipulator solving a pick-and-place task. All sensory data is provided by spike-based silicon retina cameras - eDVS (embedded Dynamic Vision Sensor) - and all reasoning and motor control is implemented in Spiking Neural Networks. For the given scenario, the robot is capable of detecting a sequence of objects blinking at different frequencies, finding one object that is not in the right place of the sequence, picking up this object and moving it to its correct position. Such a scenario demonstrates how to build large-scale networks solving a high-level cognitive task by combining several smaller networks responsible for low-level tasks. Importantly, here we focus only on generating a neural network that is capable of performing the task. This will be the basis of future work using neural network learning algorithms to improve task performance. The long-term goal is to learn sophisticated behaviours by experience while at the same time being able to introduce expert knowledge for intermediate tasks that can be used to initialize the network or to speed up the learning process.</td>
</tr>
<tr id="bib_Mirus2018a" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@article{Mirus2018a,
  author = {Florian Mirus and Cristian Axenie and Terrence C. Stewart and Jörg Conradt},
  title = {Neuromorphic Sensorimotor Adaptation for Robotic Mobile Manipulation: From Sensing to Behaviour },
  journal = {Cognitive Systems Research },
  year = {2018},
  volume = {50},
  pages = {52 -- 66},
  url = {http://www.sciencedirect.com/science/article/pii/S1389041717300955},
  doi = {10.1016/j.cogsys.2018.03.006}
}
</pre></td>
</tr>
<tr id="Mirus2016" class="entry">
	<td><strong>Mirus F</strong>, Slomian F, D&ouml;rr S, Lopez FG, Gruhler M and Pfadt J (2016), <i>"A Modular Hybrid Localization Approach for Mobile Robots Combining Local Grid Maps and Natural Landmarks"</i>, In Proceedings of the 31st Annual ACM Symposium on Applied Computing. New York, NY, USA , pp. 287-290. ACM.
	<p class="infolinks">[<a href="javascript:toggleInfo('Mirus2016','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Mirus2016','bibtex')">BibTeX</a>] [<a href="http://dx.doi.org/10.1145/2851613.2851929" target="_blank">DOI</a>] [<a href="http://doi.acm.org/10.1145/2851613.2851929" target="_blank">URL</a>]</p>
	</td>
</tr>
<tr id="abs_Mirus2016" class="abstract noshow">
	<td><b>Abstract</b>: This paper presents a hybrid localization approach for mobile robots combining local grid maps and natural landmarks. The approach at hand benefits from the advantages of both environment representations. While using memory-efficient geometric models describing natural landmarks as features for localization in structured regions, the proposed system clusters the remaining areas as raw local grid maps and incorporates those as pose features only for unstructured areas of the environment. To evaluate the functionality and performance of the approach at hand, extensive testing and benchmarking in an experimental setup has been conducted using an external sensor system for reference measurements.</td>
</tr>
<tr id="bib_Mirus2016" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{Mirus2016,
  author = {Mirus, Florian and Slomian, Frank and D&ouml;rr, Stefan and Lopez, Felipe Garcia and Gruhler, Matthias and Pfadt, J&uuml;rgen},
  title = {A Modular Hybrid Localization Approach for Mobile Robots Combining Local Grid Maps and Natural Landmarks},
  booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
  publisher = {ACM},
  year = {2016},
  pages = {287--290},
  url = {http://doi.acm.org/10.1145/2851613.2851929},
  doi = {10.1145/2851613.2851929}
}
</pre></td>
</tr>
<tr id="Beyeler2014" class="entry">
	<td>Beyeler M, <strong>Mirus F</strong> and Verl A (2014), <i>"Vision-based robust road lane detection in urban environments"</i>, In 2014 IEEE International Conference on Robotics and Automation, ICRA<br> 2014, Hong Kong, China, May 31 - June 7, 2014. , pp. 4920-4925. IEEE.
	<p class="infolinks">[<a href="javascript:toggleInfo('Beyeler2014','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Beyeler2014','bibtex')">BibTeX</a>] [<a href="http://dx.doi.org/10.1109/ICRA.2014.6907580" target="_blank">DOI</a>] [<a href="http://dx.doi.org/10.1109/ICRA.2014.6907580" target="_blank">URL</a>]</p>
	</td>
</tr>
<tr id="abs_Beyeler2014" class="abstract noshow">
	<td><b>Abstract</b>: Road and lane detection play an important role in autonomous driving and commercial driver-assistance systems. Vision-based road detection is an essential step towards autonomous driving, yet a challenging task due to illumination and complexity of the visual scenery. Urban scenes may present additional challenges such as intersections, multi-lane scenarios, or clutter due to heavy traffic. This paper presents an integrative approach to ego-lane detection that aims to be as simple as possible to enable real-time computation while being able to adapt to a variety of urban and rural traffic scenarios. The approach at hand combines and extends a road segmentation method in an illumination-invariant color image, lane markings detection using a ridge operator, and road geometry estimation using RANdom SAmple Consensus (RANSAC). Employing the segmented road region as a prior for lane markings extraction significantly improves the execution time and success rate of the RANSAC algorithm, and makes the detection of weakly pronounced ridge structures computationally tractable, thus enabling ego-lane detection even in the absence of lane markings. Segmentation performance is shown to increase when moving from a color-based to a histogram correlation-based model. The power and robustness of this algorithm has been demonstrated in a car simulation system as well as in the challenging KITTI data base of real-world urban traffic scenarios.</td>
</tr>
<tr id="bib_Beyeler2014" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{Beyeler2014,
  author = {Michael Beyeler and<br> Florian Mirus and<br> Alexander Verl},
  title = {Vision-based robust road lane detection in urban environments},
  booktitle = {2014 IEEE International Conference on Robotics and Automation, ICRA<br> 2014, Hong Kong, China, May 31 - June 7, 2014},
  publisher = {IEEE},
  year = {2014},
  pages = {4920--4925},
  url = {http://dx.doi.org/10.1109/ICRA.2014.6907580},
  doi = {10.1109/ICRA.2014.6907580}
}
</pre></td>
</tr>
<tr id="Mirus2012" class="entry">
	<td><strong>Mirus F</strong>, Pfadt J, Connette C, Ewert B, Gr&uuml;dl D and Verl A (2012), <i>"Detection of Moving and Stationary Objects at High Velocities using Cost-Efficient Sensors, Curve-Fitting and Neural Networks"</i>, In 4th Workshop on Planning, Perception and Navigation for Intelligent Vehicles, 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems IROS<br> 2012, Vilamoura, Portugal, Ocotber 7-12th, 2012.  IEEE.
	<p class="infolinks">[<a href="javascript:toggleInfo('Mirus2012','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Mirus2012','bibtex')">BibTeX</a>] [<a href="https://www.elektromobilitaet.fraunhofer.de/content/dam/elektromobilitaet/de/documents/fsem_ii/fmirus_iros12_detection_of_moving_and_stationary_objects_at_high_velocities_using_cost-efficient_sensors_curve_fitting_and_neural_networks.pdf" target="_blank">URL</a>]</p>
	</td>
</tr>
<tr id="abs_Mirus2012" class="abstract noshow">
	<td><b>Abstract</b>: In recent years, driver-assistance systems have emerged as one major possibility to increase comfort and – even more important – safety in road traffic. Still, cost is one major hindrance to the widespread use of safety systems such as lane change or blind spot warning. To facilitate the widespread adoption of such assistance systems, thus increasing safety for all traffic participants, the use of cost-efficient components is of crucial importance. This paper investigates the usage of cost-efficient, widely used ultrasonic sensors for blind spot warning at high velocities. After discussing the requirements and setup of such a system a model-based approach for the detection of moving and stationary objects is outlined. The sensor-signal is compared with a precalculated curve data base and the correlation-coefficients are feeded into a neural network. To revise its performance the concept at hand is qualitatively and quantitatively evaluated in real road traffic situations under different driving conditions.</td>
</tr>
<tr id="bib_Mirus2012" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{Mirus2012,
  author = {Florian Mirus and J&uuml;rgen Pfadt and C. Connette and Bj&ouml;rn Ewert and Dietmar Gr&uuml;dl and<br> Alexander Verl},
  title = {Detection of Moving and Stationary Objects at High Velocities using Cost-Efficient Sensors, Curve-Fitting and Neural Networks},
  booktitle = {4th Workshop on Planning, Perception and Navigation for Intelligent Vehicles, 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems IROS<br> 2012, Vilamoura, Portugal, Ocotber 7-12th, 2012},
  publisher = {IEEE},
  year = {2012}
}
</pre></td>
</tr>
<tr id="Connette2012" class="entry">
	<td>Connette C, Fischer J, Maidel B, <strong>Mirus F</strong>, Nilsson S, Pfeiffer K, Verl A, Durbec A, Ewert B, Haar T and Gruedl D (2012), <i>"Rapid Detection of Fast Objects in Highly Dynamic Outdoor Environments using Cost-Efficient Sensors"</i>, In Robotics; Proceedings of ROBOTIK 2012; 7th German Conference on., May, 2012. , pp. 1-4.
	<p class="infolinks">[<a href="javascript:toggleInfo('Connette2012','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Connette2012','bibtex')">BibTeX</a>] [<a href="https://www.ipa.fraunhofer.de/content/dam/ipa/en/documents/Expertises/Roboter--und-Assistenzsysteme/Rapid_Detection_of_Fast_Objects_in_Dynamic_Environments.pdf" target="_blank">URL</a>]</p>
	</td>
</tr>
<tr id="abs_Connette2012" class="abstract noshow">
	<td><b>Abstract</b>: In recent years driver-assistance systems have emerged as one major possibility to increase comfort and - even more important - safety in road traffic. Still, cost is one major hindrance to the widespread use of safety systems such as lane change or blind spot warning. To facilitate the widespread adoption of such safety systems, thus increasing safety for all traffic participants, the use of cost-efficient components is of crucial importance. This paper investigates the usage of cost-efficient, widely used ultrasonic sensors for blind spot warning at high velocities. The normative aspects as well as the functional requirements originating from considerations on human-machine interaction and user-experience are discussed. A fuzzy-markov-chain based approach for the detection of fast objects is outlined. The proposed procedure is implemented on a regular electronic control unit (ECU) and qualitatively and quantitatively evaluated in real road traffic scenarios.</td>
</tr>
<tr id="bib_Connette2012" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{Connette2012,
  author = {C. Connette and J. Fischer and B. Maidel and F. Mirus and S. Nilsson and K. Pfeiffer and A. Verl and A. Durbec and B. Ewert and T. Haar and D. Gruedl},
  title = {Rapid Detection of Fast Objects in Highly Dynamic Outdoor Environments using Cost-Efficient Sensors},
  booktitle = {Robotics; Proceedings of ROBOTIK 2012; 7th German Conference on},
  year = {2012},
  pages = {1-4}
}
</pre></td>
</tr>
<tr id="Mirus2011" class="entry">
	<td><strong>Mirus F</strong> (2011), <i>"Galois-Groups of relative Frobenius-Modules (original German title: Galoisgruppen von relativen Frobenius-Moduln)"</i>, diploma thesis, Heidelberg University.
	<p class="infolinks"> [<a href="javascript:toggleInfo('Mirus2011','bibtex')">BibTeX</a>] [<a href="https://fmirus.github.io/assets/docs/diploma_thesis.pdf" target="_blank">pdf</a>]</p>
	</td>
</tr>
<tr id="bib_Mirus2011" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@MastersThesis{Mirus2011,
  author    = {Florian Mirus},
  school    = {Heidelberg University},
  title     = {Galois-Gruppen von relativen Frobenius-Moduln},
  year      = {2011},
  type      = {Diploma thesis},
  timestamp = {2019.02.08},
  url       = {https://fmirus.github.io/assets/docs/diploma_thesis.pdf},
}
</pre></td>
</tr>
</tbody>
</table>
<!-- file generated by JabRef -->
</body>
</section>
</html>
